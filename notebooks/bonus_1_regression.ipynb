{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00150c96-2537-470b-855b-dfcca475778d",
   "metadata": {},
   "source": [
    "# Linear and logistic regression\n",
    "\n",
    "In this notebook, you will explore the fundamental concepts behind two key algorithms in machine learning: linear regression and logistic regression.\n",
    "These algorithms are widely used for prediction and classification tasks, including many biomedical applications.\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "- Understand the principles of linear and logistic regression.\n",
    "- Implement linear regression from scratch using the normal equation in NumPy.\n",
    "- Apply linear regression on a real biomedical dataset using scikit-learn.\n",
    "- Implement logistic regression from scratch using NumPy with an optimization method.\n",
    "- Apply logistic regression on a biomedical classification dataset using scikit-learn.\n",
    "- Evaluate and interpret the performance of both regression models.\n",
    "\n",
    "We will start by implementing these methods from scratch to understand their inner workings, and then we will move to using the scikit-learn library for real-world applications.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "You will need the following Python packages to complete this notebook:\n",
    "\n",
    "- [NumPy](https://numpy.org/): for numerical computing.\n",
    "- [Matplotlib](https://matplotlib.org/): for data visualization.\n",
    "- [SciPy](https://scipy.org/): for optimization in logistic regression.\n",
    "- [scikit-learn](https://scikit-learn.org/): for applying pre-built machine learning models.\n",
    "\n",
    "You can install these packages by running:\n",
    "\n",
    "```\n",
    "pip install numpy matplotlib scipy scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4a23b6-6877-4869-b9eb-f1f217d4aa9a",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "Linear regression is a supervised machine learning algorithm used to predict a continuous target variable $y$ based on one or more input features $X$.\n",
    "It is one of the simplest and most interpretable algorithms for regression tasks.\n",
    "The idea behind linear regression is to find a linear relationship between the input features and the target variable.\n",
    "\n",
    "The equation for a simple linear regression model with one feature looks like this:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\beta_0$ is the intercept (also called bias),\n",
    "- $\\beta_1$ is the slope (or weight) of the feature,\n",
    "- $x$ is the input feature,\n",
    "- $y$ is the predicted output.\n",
    "\n",
    "The goal of linear regression is to find the optimal values of the parameters $\\beta_0$ and $\\beta_1$ (collectively referred to as $\\beta$) that minimize the error between the predicted values and the actual values.\n",
    "The error is often quantified using **mean squared error (MSE)**, which is the average squared difference between the predicted values and the actual values:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $n$ is the number of data points,\n",
    "- $y_i$ is the actual value of the $i$-th data point,\n",
    "- $\\hat{y}_i$ is the predicted value of the $i$-th data point.\n",
    "\n",
    "A lower MSE indicates a better fit between the model and the data.\n",
    "\n",
    "### Solving linear regression with the normal equation\n",
    "\n",
    "Although not discussed in class, one way to find the optimal values for $\\beta$ is the **normal equation**.\n",
    "This is particularly useful for small datasets where computation is not too expensive.\n",
    "The normal equation is derived by setting the derivative of the MSE with respect to the parameters $\\beta$ to zero and solving for $\\beta$.\n",
    "\n",
    "The normal equation is given by:\n",
    "\n",
    "$$\n",
    "\\beta = (X^T X)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $X$ is the design matrix, which includes the input features (and an additional column of 1's for the intercept term),\n",
    "- $X^T$ is the transpose of $X$,\n",
    "- $y$ is the vector of target values,\n",
    "- $\\beta$ is the vector of parameters (including the intercept and slope).\n",
    "\n",
    "**How does it work?**\n",
    "\n",
    "1. **Features**: The matrix $X$ consists of all the input data. For a simple case with one feature, $X$ will be a matrix where each row corresponds to a data point, and each column corresponds to a feature. An additional column of 1's is added to account for the intercept term $\\beta_0$.\n",
    "   \n",
    "2. **Matrix multiplication**: The term $X^T X$ multiplies the matrix $X$ by its transpose. This results in a square matrix that helps to scale the solution properly.\n",
    "\n",
    "3. **Inverse of the matrix**: The term $(X^T X)^{-1}$ is the inverse of the matrix $X^T X$, which adjusts the solution to find the best-fitting parameters.\n",
    "\n",
    "4. **Final calculation**: The multiplication of $(X^T X)^{-1}$ with $X^T y$ gives us the optimal values of the parameters $\\beta$.\n",
    "\n",
    "**Why use the normal equation?**\n",
    "\n",
    "The normal equation is a direct solution method, which means we don't need to use iterative optimization methods like gradient descent.\n",
    "This can be advantageous when:\n",
    "- The dataset is relatively small (because matrix inversion can be computationally expensive for large datasets),\n",
    "- We want a quick solution without worrying about learning rates and convergence issues.\n",
    "\n",
    "However, it becomes impractical for very large datasets where $X^T X$ becomes too large to invert efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62f43f1-6d49-4b3a-9d1a-7f5da2776ffa",
   "metadata": {},
   "source": [
    "### Task: Linear regression from scratch using NumPy\n",
    "\n",
    "Now that we've discussed the theory, let's implement linear regression using the normal equation in practice.\n",
    "\n",
    "**Steps**:\n",
    "\n",
    "1. **Generate a synthetic dataset**: We'll create a dataset where the patient's age (input feature) influences their blood pressure (target variable), with some random noise added to make it realistic.\n",
    "2. **Add an intercept term**: Modify the dataset to include an intercept term for the bias $\\beta_0$.\n",
    "3. **Compute the optimal parameters**: Use the normal equation to calculate the optimal values of $\\beta$.\n",
    "4. **Make predictions**: Use the optimal parameters to make predictions on the dataset.\n",
    "5. **Calculate the MSE**: Evaluate the model by calculating the MSE between the predicted values and the actual values.\n",
    "6. **Visualize the results**: Plot the original data and the regression line to visualize how well the model fits the data.\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "- Generate the synthetic data using NumPy.\n",
    "- Implement the normal equation in code using NumPy's matrix operations.\n",
    "- Calculate the MSE to assess the accuracy of the model.\n",
    "- Plot the results to show the regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9dabc4-8cf4-4e05-b182-49f69d02d5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Generate synthetic dataset (age vs blood pressure).\n",
    "np.random.seed(0)\n",
    "X = np.random.randint(low=0, high=100, size=100)  # Ages between 0 and 100.\n",
    "noise = np.random.standard_normal(size=100) * 10\n",
    "y = 50 + 3 * X + noise  # Linear relationship with noise.\n",
    "\n",
    "# Add intercept term to X.\n",
    "# TODO\n",
    "X_b = None\n",
    "\n",
    "# Normal equation implementation.\n",
    "# TODO\n",
    "beta = None\n",
    "# Print the optimal parameters (intercept and slope).\n",
    "print(f\"Optimal parameters (beta): {beta}\")\n",
    "\n",
    "# Predictions using the calculated parameters.\n",
    "# TODO\n",
    "y_pred = None\n",
    "\n",
    "# Calculate the MSE.\n",
    "# TODO\n",
    "mse = None\n",
    "print(f\"MSE: {mse}\")\n",
    "\n",
    "# Plot the result.\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, y, color=\"blue\", label=\"Data\")\n",
    "ax.plot(X, y_pred, color=\"red\", label=\"Regression line\")\n",
    "ax.set_xlabel(\"Age\")\n",
    "ax.set_ylabel(\"Blood Pressure\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Linear regression: Age vs blood pressure\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c3b017-dbc0-4f61-8e6d-50cfc74e61ca",
   "metadata": {},
   "source": [
    "### Task: Linear regression on a biomedical dataset using scikit-learn\n",
    "\n",
    "Now, let's apply linear regression to a real-world dataset using **scikit-learn**.\n",
    "We will use the **diabetes dataset**, a classic dataset that contains various medical predictors and a target variable that represents disease progression.\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "- Load the diabetes dataset from scikit-learn.\n",
    "- Split the dataset into training and test sets. Tip: consult the scikit-learn documentation to find how to do this with a single function call.\n",
    "- Fit a linear regression model to predict the progression of diabetes based on the provided features.\n",
    "- Evaluate the model using MSE and visualize the predicted vs actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e16a8a0-fecd-41b5-b1ba-859fe0d28c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes\n",
    "# TODO: Additional imports.\n",
    "\n",
    "\n",
    "# Load dataset.\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "# Split into training and test sets.\n",
    "# TODO\n",
    "X_train, X_test, y_train, y_test = None\n",
    "\n",
    "# Fit linear regression model.\n",
    "# TODO\n",
    "model = None\n",
    "\n",
    "# Predict on test set.\n",
    "# TODO\n",
    "y_pred = None\n",
    "\n",
    "# Calculate the MSE.\n",
    "# TODO\n",
    "mse = None\n",
    "print(f\"MSE: {mse}\")\n",
    "\n",
    "# Plot predicted vs actual values.\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(y_test, y_pred)\n",
    "ax.plot((lim := (y_test.min(), y_test.max())), lim, c=\"black\", ls=\"--\")\n",
    "ax.set_xlabel(\"True values\")\n",
    "ax.set_ylabel(\"Predictions\")\n",
    "ax.set_title(\"Predicted vs actual diabetes progression\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b983db-5fcb-43f3-b450-28d32a7c7d23",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Logistic regression is a supervised machine learning algorithm used for **binary classification** tasks.\n",
    "It predicts the probability that a given input point belongs to one of two possible classes.\n",
    "Instead of predicting a continuous output (like linear regression), logistic regression predicts a probability value between 0 and 1.\n",
    "This makes it ideal for tasks like determining whether a patient has a particular disease (yes/no) based on some medical features.\n",
    "\n",
    "To achieve this, logistic regression uses a special function called the **sigmoid function** (also known as the logistic function).\n",
    "The sigmoid function maps any real-valued number into a value between 0 and 1, which can then be interpreted as a probability.\n",
    "\n",
    "The sigmoid function is defined as:\n",
    "\n",
    "$$h_\\beta(x) = \\frac{1}{1 + e^{-\\beta^T x}}$$\n",
    "\n",
    "Where:\n",
    "- $h_\\beta(x)$ is the predicted probability that the output is 1 (e.g., a patient has the disease),\n",
    "- $\\beta$ is the vector of parameters (weights) to be learned,\n",
    "- $x$ is the input feature vector,\n",
    "- $e$ is Euler’s number (approximately 2.718).\n",
    "\n",
    "Note that this is a slightly different but equal formulation than in the slides. (Verify that this is the case!)\n",
    "\n",
    "The sigmoid function produces an \"S\"-shaped curve that asymptotically approaches 0 and 1 as the input becomes large negative or large positive, respectively.\n",
    "\n",
    "### Binary classification\n",
    "\n",
    "Logistic regression outputs a probability between 0 and 1, but we typically classify the result into one of two classes based on a threshold (usually 0.5):\n",
    "- If the probability is greater than or equal to 0.5, we predict class 1 (e.g., the patient has the disease).\n",
    "- If the probability is less than 0.5, we predict class 0 (e.g., the patient does not have the disease).\n",
    "\n",
    "### Cost function for logistic regression\n",
    "\n",
    "The cost function used in logistic regression is based on the concept of **maximum likelihood estimation**.\n",
    "The objective is to maximize the likelihood of the observed data given the model's parameters.\n",
    "In practice, we minimize the negative log-likelihood, which gives us the following **binary cross-entropy loss** (also known as the log-loss):\n",
    "\n",
    "$$\n",
    "J(\\theta) = - \\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(h_\\beta(x_i)) + (1 - y_i) \\log(1 - h_\\beta(x_i)) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $n$ is the number of training examples,\n",
    "- $y_i$ is the actual label for the $i$-th training example (0 or 1),\n",
    "- $h_\\beta(x_i)$ is the predicted probability for the $i$-th example,\n",
    "- $\\beta$ is the vector of parameters (weights) that we want to optimize.\n",
    "\n",
    "The cost function penalizes incorrect predictions more heavily when the model is confident (i.e., when $h_\\beta(x)$ is close to 0 or 1).\n",
    "\n",
    "In contrast to linear regression (where we have a closed-form solution via the normal equation), logistic regression does not have a closed-form solution for the optimal parameters.\n",
    "Instead, we use optimization techniques like **gradient descent** to minimize the cost function and find the best parameters.\n",
    "For this exercise, we will use a built-in optimization function from the **SciPy** library to avoid needing to manually compute gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea10277-cb6b-4d0a-aab1-69fb1a676dcb",
   "metadata": {},
   "source": [
    "### Task: Logistic regression from scratch using NumPy\n",
    "\n",
    "In this task, you will implement logistic regression from scratch.\n",
    "The steps include defining the sigmoid function, the cost function (binary cross-entropy), and using the `scipy.optimize.minimize` function to find the optimal parameters.\n",
    "\n",
    "**Steps**:\n",
    "\n",
    "1. **Generate a synthetic dataset**: Create a dataset where we classify whether a patient has a disease based on two features (e.g., age and biomarker levels).\n",
    "2. **Implement the sigmoid function**: Implement the logistic function that transforms linear combinations of the input features into probabilities.\n",
    "3. **Define the cost function**: Implement the binary cross-entropy loss function.\n",
    "4. **Optimize the parameters**: Use the `scipy.optimize.minimize` function to find the best parameters that minimize the cost function.\n",
    "5. **Visualize the results**: Plot the decision boundary and the classified points to see how well the model separates the two classes.\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "- Implement the sigmoid function using NumPy.\n",
    "- Implement the cost function for logistic regression.\n",
    "- Use `scipy.optimize.minimize` to find the optimal parameters.\n",
    "- Visualize the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e22f36d-6ff0-4aee-881f-6f48f5f64fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "# Sigmoid function.\n",
    "def sigmoid(z):\n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "\n",
    "# Binary cross-entropy cost function.\n",
    "def compute_cost(beta, X, y):\n",
    "    n = len(y)\n",
    "    z = X.dot(theta)\n",
    "    predictions = sigmoid(z)\n",
    "    # Clip predictions to avoid log(0).\n",
    "    epsilon = 1e-5\n",
    "    predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "    # TODO\n",
    "    cost = None\n",
    "    return cost\n",
    "\n",
    "\n",
    "# Function for optimization (gradient calculation is handled internally by\n",
    "# minimize).\n",
    "def fit_logistic(X, y):\n",
    "    initial_beta = np.zeros(X.shape[1])\n",
    "    result = minimize(compute_cost, initial_beta, args=(X, y))\n",
    "    return result.x\n",
    "\n",
    "\n",
    "# Predict function (returns class 0 or 1).\n",
    "def predict(X, beta):\n",
    "    # TODO\n",
    "    pass\n",
    "\n",
    "\n",
    "# Generate synthetic binary classification dataset.\n",
    "np.random.seed(0)\n",
    "# Two features: age and biomarker level.\n",
    "age = np.random.random_sample(100) * 100  # Ages between 0 and 100.\n",
    "biomarker = np.random.random_sample(100) * 10  # Biomarker level.\n",
    "# Class: disease status (with some noise).\n",
    "noise = np.random.random_sample(100) * 10\n",
    "y = (3 * age + biomarker + noise > 150).astype(int)  # Binary label (0 or 1).\n",
    "y = y.flatten()\n",
    "\n",
    "# Combine age and biomarker into a single feature matrix\n",
    "# and add the intercept term.\n",
    "# TODO\n",
    "X_b = None\n",
    "\n",
    "# Fit logistic regression model.\n",
    "# TODO\n",
    "beta = None\n",
    "\n",
    "# Predict on the dataset.\n",
    "# TODO\n",
    "y_pred = None\n",
    "\n",
    "# Evaluate the model: calculate accuracy.\n",
    "accuracy = np.mean(y_pred == y)\n",
    "print(f\"Training accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Visualize the decision boundary.\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(age, biomarker, c=y_pred, label=\"Data\")\n",
    "ymin, ymax = ax.get_ylim()\n",
    "# Calculate the decision boundary (beta_0 + beta_1 * age + beta_2 * biomarker = 0).\n",
    "x_values = [age.min(), age.max()]\n",
    "y_values = -(beta[0] + np.dot(beta[1], x_values)) / beta[2]\n",
    "ax.plot(x_values, y_values, color=\"red\", label=\"Decision boundary\")\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.set_xlabel(\"Age\")\n",
    "ax.set_ylabel(\"Biomarker\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Logistic regression: Age vs biomarker\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23716c-9b32-4f27-91ea-cb2f8cede4af",
   "metadata": {},
   "source": [
    "### Task: Logistic regression on a biomedical dataset using scikit-learn\n",
    "\n",
    "Now that you understand how logistic regression works under the hood, we will apply it to a real-world biomedical dataset using the scikit-learn library.\n",
    "We'll use the **Breast Cancer Dataset** to predict whether a tumor is malignant or benign based on various medical features.\n",
    "\n",
    "**Steps**:\n",
    "\n",
    "- Load the Breast Cancer Dataset: Use scikit-learn's `load_breast_cancer` dataset.\n",
    "- Split the data: Divide the data into training and test sets.\n",
    "- Train a logistic regression model: Use scikit-learn's `LogisticRegression` to train a model.\n",
    "- Evaluate the model: Calculate the accuracy, confusion matrix, and plot the ROC curve to evaluate model performance.\n",
    "\n",
    "**Expected outputs**:\n",
    "\n",
    "- Optimal parameters: The optimal weights (parameters) for the logistic regression model.\n",
    "- Accuracy: The accuracy score of the model on the test dataset.\n",
    "- Confusion matrix: A table showing the number of correct and incorrect predictions for each class.\n",
    "- ROC curve: A plot showing the trade-off between the true positive rate and false positive rate for different classification thresholds.\n",
    "\n",
    "In this task, you will see how logistic regression can be applied to real-world biomedical data and understand the model's performance through various evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39168fc-bde7-4647-9ea9-af6e8b1fad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "# TODO: Additional imports.\n",
    "\n",
    "\n",
    "# Load the Breast Cancer dataset.\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data into training and test sets.\n",
    "# TODO\n",
    "X_train, X_test, y_train, y_test = None\n",
    "\n",
    "# Train the logistic regression model\n",
    "# TODO\n",
    "model = None\n",
    "\n",
    "# Predict on the test set\n",
    "# TODO\n",
    "y_pred = None\n",
    "\n",
    "# Evaluate the model: calculate accuracy on the _test_ set.\n",
    "# TODO\n",
    "accuracy = None\n",
    "print(f\"Test accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Confusion matrix on the _test_ set.\n",
    "# TODO\n",
    "cm = None\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "# ROC curve.\n",
    "# TODO\n",
    "fpr, tpr = None\n",
    "roc_auc = None\n",
    "\n",
    "# Plot ROC curve.\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "ax.plot([0, 1], [0, 1], c=\"black\", ls=\"--\")\n",
    "ax.set_xlabel(\"False positive rate\")\n",
    "ax.set_ylabel(\"True positive rate\")\n",
    "ax.set_title(\"Receiver operating characteristic curve\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
