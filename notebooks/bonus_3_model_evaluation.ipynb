{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e4e3c30-a9ce-4c2e-aee7-c5345723cce5",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "\n",
    "In this notebook, you will explore key techniques for evaluating machine learning models.\n",
    "Specifically, you will focus on:\n",
    "\n",
    "- Manually calculating performance metrics from a confusion matrix.\n",
    "- Understanding the differences between ROC and PR curves, especially when dealing with imbalanced data.\n",
    "- Identifying and mitigating the issue of data leakage when performing feature selection.\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "By the end of this notebook, you should be able to:\n",
    "\n",
    "- Calculate and interpret key performance metrics such as accuracy, precision, recall, and F1-score.\n",
    "- Understand the significance of ROC and PR curves and when to use each.\n",
    "- Identify data leakage and prevent it by applying proper cross-validation strategies.\n",
    "\n",
    "## Model evaluation recap\n",
    "\n",
    "Before we dive into the exercises, letâ€™s briefly review the key concepts that are essential for evaluating machine learning models.\n",
    "These concepts will guide you through the tasks you will be working on.\n",
    "\n",
    "### Train-validation-test split\n",
    "\n",
    "Properly evaluating your machine learning model is essential to assess it performance.\n",
    "Importantly, we want to model to be **generalizable**, i.e. it performs well on unseen (future) data.\n",
    "\n",
    "Typically, the data is split into a **train and test set**.\n",
    "The model is then trained on the former, and its performance is evaluated in the end on the latter.\n",
    "It is very important to only evaluate performance of your model on the test set as the very last step.\n",
    "In case the test performance is used to direct model changes, it is no longer independent and **overfitting** will occur.\n",
    "\n",
    "In case your model has hyperparameters that can be tuned, the data is split into a third set, called the **validation set**.\n",
    "The validation set can then be used to assess the performance for different hyperparameter values to keep the test set separate for the final performance evaluation.\n",
    "\n",
    "### Cross-validation\n",
    "\n",
    "Cross-validation is an essential technique used to assess model performance more reliably.\n",
    "It involves splitting the data into multiple subsets (folds), training the model on some folds, and testing it on the remaining folds.\n",
    "The most common form is **k-fold cross-validation**, where the data is split into `k` folds, and the model is trained `k` times, each time using a different fold for testing.\n",
    "\n",
    "Cross-validation helps mitigate overfitting by ensuring the model generalizes well to unseen data.\n",
    "\n",
    "### Bootstrap\n",
    "\n",
    "Bootstrap is another resampling technique used to assess the uncertainty in model performance.\n",
    "It involves repeatedly sampling the data (with replacement) to create new training sets, training the model on these sets, and evaluating its performance on the unsampled data.\n",
    "Bootstrap helps to estimate the variance of the model's performance metrics.\n",
    "\n",
    "### Confusion matrix & performance metrics\n",
    "\n",
    "The **confusion matrix** is a 2x2 table used to describe the performance of a classification model.\n",
    "The entries are:\n",
    "\n",
    "- **True positives (TP)**: The model correctly predicts the positive class.\n",
    "- **False positives (FP)**: The model incorrectly predicts the positive class.\n",
    "- **True negatives (TN)**: The model correctly predicts the negative class.\n",
    "- **False negatives (FN)**: The model incorrectly predicts the negative class.\n",
    "\n",
    "From the confusion matrix, you can derive several performance metrics:\n",
    "\n",
    "- **Accuracy**: The proportion of correct predictions.\n",
    "- **Precision**: The proportion of positive predictions that are actually positive.\n",
    "- **Recall (sensitivity)**: The proportion of actual positives that are correctly predicted.\n",
    "- **F1-Score**: The harmonic mean of precision and recall, useful when classes are imbalanced.\n",
    "\n",
    "### **ROC and PR curves**\n",
    "\n",
    "The **receiver operating characteristic (ROC) curve** plots the true positive rate (recall) against the false positive rate at various threshold settings.\n",
    "The **area under the ROC curve (AUROC)** is commonly used to summarize the model's performance, with 1.0 indicating a perfect classifier and 0.5 indicating a random classifier.\n",
    "\n",
    "The **precision-recall (PR) curve** plots precision against recall at various thresholds.\n",
    "PR curves are particularly useful when dealing with imbalanced data, where the positive class is much smaller than the negative class.\n",
    "The **average precision (AP)** (approximately equivalent to the area under the PR curve) summarizes the trade-off between precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d3840-0932-4b01-a2b7-94df10a94713",
   "metadata": {},
   "source": [
    "## Task: Calculate performance metrics from a confusion matrix\n",
    "\n",
    "In this task, you will:\n",
    "\n",
    "- Train a logistic regression model on a simple binary classification dataset.\n",
    "- Generate a confusion matrix based on the predictions.\n",
    "- Manually calculate key performance metrics: accuracy, precision, recall, and F1-score.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "- Create the dataset: Create a binary classification dataset with 1,000 samples and 20 features.\n",
    "- Split the data: Divide the data into training and test sets.\n",
    "- Train a logistic regression model: Use scikit-learn's `LogisticRegression` to train a model.\n",
    "- Evaluate the model: Make predictions on the test set and compute the confusion matrix. Using the values from the confusion matrix, calculate the following metrics manually:\n",
    "    - Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    - Precision = TP / (TP + FP)\n",
    "    - Recall = TP / (TP + FN)\n",
    "    - F1-score = 2 * (Precision * Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d01de4-1b4d-4eb6-8f31-62982d5033e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Generate synthetic dataset.\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "# TODO\n",
    "X_train, X_test, y_train, y_test = None\n",
    "\n",
    "# Train logistic regression model.\n",
    "model = None\n",
    "\n",
    "# Generate confusion matrix.\n",
    "# TODO\n",
    "y_pred = None\n",
    "cm = None\n",
    "\n",
    "# Calculate performance metrics.\n",
    "# TODO\n",
    "accuracy = None\n",
    "precision = None\n",
    "recall = None\n",
    "f1_score = None\n",
    "\n",
    "# Print the results.\n",
    "print(f\"Confusion matrix: \\n{cm}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c3b017-dbc0-4f61-8e6d-50cfc74e61ca",
   "metadata": {},
   "source": [
    "## Task: ROC vs PR curves for imbalanced data\n",
    "\n",
    "In biomedical applications, it is common to encounter imbalanced datasets where the number of positive cases (e.g., patients with a rare disease) is significantly smaller than the number of negative cases (e.g., healthy patients).\n",
    "For example, in cancer detection, only a small fraction of patients might actually have cancer, making the dataset highly skewed toward healthy individuals.\n",
    "\n",
    "This class imbalance can lead to misleading performance metrics when evaluating classifiers.\n",
    "For instance, accuracy can be very high if the model simply predicts the majority class, but it would fail to detect the minority class, which is often the class of interest in medical diagnostics.\n",
    "\n",
    "In this task, you will:\n",
    "\n",
    "- Simulate a highly imbalanced dataset to mimic real-world scenarios in biomedicine, such as disease diagnosis where the positive cases (disease presence) are much rarer than negative cases (no disease).\n",
    "- Train a logistic regression model and generate ROC and PR curves.\n",
    "- Analyze the differences between ROC and PR curves for imbalanced datasets, and explain which is more appropriate for imbalanced biomedical problems like rare disease detection.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "- Generate an imbalanced dataset: Create a dataset with 1,000 samples and 20 features, where 90% of the samples belong to the negative class (representing healthy individuals), and 10% belong to the positive class (representing diseased individuals).\n",
    "- Split the data: Divide the data into training and test sets.\n",
    "- Train a logistic regression model: Use scikit-learn's `LogisticRegression` to train a model.\n",
    "- Plot the ROC curve: Use `roc_curve` from `sklearn.metrics` to plot the ROC curve.\n",
    "- Plot the PR curve: Use `precision_recall_curve` from `sklearn.metrics` to plot the PR curve. On each curve, draw a dashed line indicating random performance.\n",
    "- Compare the curves: Explain why the PR curve may give more insight than the ROC curve in biomedical problems with imbalanced datasets, where detecting the minority class (diseased patients) is more important than simply maximizing overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65c12fd-59dd-4f36-b146-01fb4ae87169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc, average_precision_score, roc_curve\n",
    "\n",
    "\n",
    "# Generate imbalanced dataset.\n",
    "# TODO\n",
    "X, y = None\n",
    "X_train, X_test, y_train, y_test = None\n",
    "\n",
    "# Train logistic regression model.\n",
    "# TODO\n",
    "model = None\n",
    "\n",
    "# Predict on the test set.\n",
    "# TODO\n",
    "y_proba = None\n",
    "\n",
    "# Plot ROC and PR curves.\n",
    "# TODO\n",
    "fpr, tpr, _ = None\n",
    "roc_auc = None\n",
    "precision, recall, _ = None\n",
    "ap = None\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12.8, 4.8))\n",
    "\n",
    "axes[0].plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "axes[0].set_xlim(-0.05, 1.05)\n",
    "axes[0].set_ylim(-0.05, 1.05)\n",
    "axes[0].set_xlabel(\"False positive rate\")\n",
    "axes[0].set_ylabel(\"True positive rate\")\n",
    "axes[0].set_title(\"ROC curve\")\n",
    "axes[0].legend(loc=\"lower right\")\n",
    "\n",
    "axes[1].plot(recall, precision, label=f\"PR curve (AP = {ap:.2f})\")\n",
    "axes[1].set_xlim(-0.05, 1.05)\n",
    "axes[1].set_ylim(-0.05, 1.05)\n",
    "axes[1].set_xlabel(\"Recall\")\n",
    "axes[1].set_ylabel(\"Precision\")\n",
    "axes[1].set_title(\"PR curve\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Draw dashed lines indicating random performance.\n",
    "# TODO\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Analyze the differences between ROC and PR curves for imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea10277-cb6b-4d0a-aab1-69fb1a676dcb",
   "metadata": {},
   "source": [
    "## Task: Data leakage due to feature selection outside of cross-validation\n",
    "\n",
    "In biomedical research, especially when working with molecular data (e.g., gene expression, proteomics, or metabolomics), it is common to have datasets with many more features (e.g., thousands of genes, proteins, or metabolites) than samples (e.g., dozens to hundreds of patients).\n",
    "For example, when developing a classifier for diagnostic purposes, such as distinguishing between patients with and without a disease based on their molecular profiles, the feature set can be vast, but the number of samples may be limited due to the difficulty and cost of collecting biological data.\n",
    "\n",
    "Feature selection is a crucial step in such situations, as models trained on all available features might suffer from overfitting due to the high dimensionality of the data.\n",
    "The goal of feature selection is to reduce the feature space to only the most informative features, improving model generalization and interpretability.\n",
    "\n",
    "There are different approaches to feature selection:\n",
    "\n",
    "- Filter methods: Select features based on statistical measures, such as correlation or mutual information with the target variable, before training the model.\n",
    "- Wrapper methods: Select features by evaluating model performance on different subsets of features, iteratively optimizing the subset.\n",
    "- Embedded methods: Perform feature selection as part of the model training process, such as Lasso (L1 regularization).\n",
    "\n",
    "For this task, we will use **filter-based feature selection** with the ANOVA F-test, which selects features based on their ability to differentiate between the classes.\n",
    "However, it is important to perform this selection properly to avoid data leakage.\n",
    "\n",
    "If feature selection is done **before** cross-validation, information about the test data can \"leak\" into the training process, resulting in overly optimistic performance estimates.\n",
    "This is a common mistake in molecular data analysis and even the scientific literature, leading to classifiers that appear to work well but fail when applied to new, unseen data.\n",
    "\n",
    "In this task, you will:\n",
    "\n",
    "- Implement a naive feature selection approach, where feature selection is done outside of cross-validation (incorrect), and observe inflated performance.\n",
    "- Implement the correct feature selection approach, where feature selection is applied within cross-validation, and compare the results.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "- Simulate data: Generate a random dataset with 100 samples and 10,000 features. Such a large number of features mimics real-world molecular datasets where the majority of features (genes, proteins, metabolites) may not be relevant to the classification task.\n",
    "- Perform naive feature selection (outside cross-dalidation):\n",
    "    - Use `SelectKBest` with the ANOVA F-test to select the top 50 features **before** cross-validation.\n",
    "    - Train a logistic regression model using these selected features and evaluate its performance using cross-validation.\n",
    "    - Compare the cross-validation scores and observe that the performance will likely appear better than random, even though the data is purely random.\n",
    "- Perform correct feature selection (inside cross-dalidation):\n",
    "    - Use a `Pipeline` to combine feature selection and logistic regression, ensuring that feature selection happens inside cross-validation, preventing data leakage.\n",
    "    - Compare the cross-validation scores with the naive approach and observe that the performance is now close to random, as expected with random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e22f36d-6ff0-4aee-881f-6f48f5f64fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Generate random dataset.\n",
    "# 100 samples, 10,000 features (many more features than samples).\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 10000)  # 100 samples, 10,000 random features.\n",
    "y = np.random.randint(0, 2, 100)  # Random binary labels (0 or 1).\n",
    "\n",
    "# Define cross-validation scheme.\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Naive feature selection outside CV (incorrect).\n",
    "# Select the top 50 features before cross-validation.\n",
    "# TODO\n",
    "selector = None\n",
    "X_selected = None\n",
    "\n",
    "# Train a logistic regression model and\n",
    "# evaluate performance using cross-validation.\n",
    "model = LogisticRegression()\n",
    "scores_naive = cross_val_score(model, X_selected, y, cv=cv)\n",
    "\n",
    "# Print the performance.\n",
    "print(f\"Naive feature selection (outside CV) accuracy: {np.mean(scores_naive):.2f}\")\n",
    "\n",
    "# Correct feature selection inside CV.\n",
    "# Use a Pipeline to ensure feature selection is done inside each CV fold.\n",
    "# TODO\n",
    "pipeline = None\n",
    "\n",
    "# Evaluate performance using cross-validation.\n",
    "scores_correct = cross_val_score(pipeline, X, y, cv=cv)\n",
    "\n",
    "# Print the performance.\n",
    "print(f\"Correct feature selection (inside CV) accuracy: {np.mean(scores_correct):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec1d3b7-2e7e-4e42-b4f8-16c728acfda3",
   "metadata": {},
   "source": [
    "**Expected outcome**\n",
    "\n",
    "When feature selection is done outside of cross-validation, you may observe an accuracy much higher than expected, even though the dataset is purely random.\n",
    "This happens because information about the test set \"leaks\" into the training process, causing the model to appear better than it really is.\n",
    "\n",
    "When feature selection is done inside cross-validation (i.e., the correct way), the accuracy should drop significantly, reflecting the fact that the dataset is random and the model cannot find meaningful patterns.\n",
    "The performance will be closer to random guessing (around 50%).\n",
    "\n",
    "Remember the **prime directive of machine learning**:\n",
    "\n",
    "> Only evaluate your models on data that has never been used for training.\n",
    "\n",
    "Performing feature selection on the full dataset violates this rule, even though we subsequently use cross-validation to train and evaluate our model.\n",
    "This leads to data leakage, which is a subtle but common issue.\n",
    "In this case, because we used only statistically significant features for model training, class information leaked to our test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
