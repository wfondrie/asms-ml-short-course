{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FOvWLgEcSRi"
   },
   "source": [
    "# Predicting Peptide Collisional Crossection from timsTOF data\n",
    "\n",
    "In this lab, we will build a Transformer model to predict the measured collisional cross section (CCS) of a peptide from its sequence and charge state, using subsets of the original training and test data from [Meier et al](https://www.nature.com/articles/s41467-021-21352-8).\n",
    "To accomplish this task, we'll create a Transformer encoder for peptide sequences and charge states, and add a feed forward neural network to predict the CCS of each peptide.\n",
    "\n",
    "This lab makes use of Depthcharge, a Python package that Wout and Will have written to model mass spectrometry data with neural networks. Depthcharge provides nice building blocks for us to use within the PyTorch deep learning framework to build these models.\n",
    "\n",
    "**Before proceeding with this notebook, make sure to switch a GPU runtime on Google Colab.** To do this, click `Runtime` -> `Change runtime type`, and select `GPU` under `Hardware accelerator`. If you have a box called `GPU type` we recommend selecting the `T4` GPU to run this notebook previously.\n",
    "\n",
    "## Setup\n",
    "\n",
    "The follow code installs the additional dependencies we'll need: Depthcharge, PyTorch Lightning, and Tensorboard. \n",
    "It also downloads the data that we'll be using, directly from the code repository from Meier et al, [here](https://github.com/theislab/DeepCollisionalCrossSection).\n",
    "In the end, we are left with our data in the working directory, `combined_sm.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCj-fMscvwej"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "pip install lightning tensorboard git+https://github.com/wfondrie/depthcharge.git@asms \n",
    "wget -nc https://github.com/theislab/DeepCollisionalCrossSection/raw/master/data/combined_sm.csv.tar.gz\n",
    "tar -xzvf combined_sm.csv.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKN6jkPdcSRk"
   },
   "source": [
    "## Import the libraries we'll need\n",
    "To work with our data, we'll need a handful of standard data science tools (NumPy, Pandas, etc.).\n",
    "For model building, we'll use PyTorch Lightning to wrap our model from Depthcharge, making it easy to train.\n",
    "\n",
    "From Depthcharge, we'll use the following classes:\n",
    "- `PeptideDataset` - This is a PyTorch Dataset that is designed to hold peptide sequences, their charge states, and additional metadata.\n",
    "- `FeedForward` - This is a utility PyTorch Module for quickly creating feed forward neural networks.\n",
    "- `PeptideTokenizer` - This class defines the amino acid alphabet, including modifications, that are valid tokens for our model. \n",
    "  It also tells Depthcharge how to convert a peptide sequence into tokens and back. \n",
    "  First-class support for ProForma formatted peptide sequences is included out-of-the-box.\n",
    "- `PeptideTransformerEncoder` - This is a PyTorch Module that embeds the peptide and its residues using a Transformer architecture.\n",
    "\n",
    "After importing these libraries, the following code also sets a plotting theme and a random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-N6rbVrnwVNL",
    "outputId": "dcfa4592-9c1b-4278-bc94-73f10ae1f290"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from depthcharge.data import PeptideDataset\n",
    "from depthcharge.feedforward import FeedForward\n",
    "from depthcharge.tokenizers import PeptideTokenizer\n",
    "from depthcharge.transformers import PeptideTransformerEncoder\n",
    "\n",
    "# Set our plotting theme:\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "# Set random seeds\n",
    "pl.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLY2ThwfcSRl"
   },
   "source": [
    "## Parse the data\n",
    "With our library loaded, we can now parse the CSV file from Meier et al, and sample only 10% of the peptides from the dataset for our lab. \n",
    "The peptide sequences are provided in a MaxQuant format, which we convert to be ProForma compliant.\n",
    "\n",
    "We then try and split the data in to training, validation, and test splits, matching the test data to that described in the paper;\n",
    "The paper states that the ProteomeTools subset was used as a test set, which are denoted by using the `PT` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "u8azBgm55Aln",
    "outputId": "4a0eaa25-b3f1-4018-8034-81178da84351",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = (\n",
    "    pd.read_csv(\"combined_sm.csv\", index_col=0)     # Read the data\n",
    "    .sample(frac=0.1)                               # Use only 10% of the data\n",
    "    .reset_index()                                  # Renumber rows\n",
    "    .rename(columns={\"Modified sequence\": \"Seq\"})   # Make the column shorter\n",
    ")\n",
    "\n",
    "# Convert sequences to be ProForma-compliant.\n",
    "data[\"Seq\"] = (\n",
    "    data[\"Seq\"]\n",
    "    .str.replace(\"_(ac)\", \"[Acetyl]-\", regex=False)\n",
    "    .str.replace(\"M(ox)\", \"M[Oxidation]\", regex=False)\n",
    "    .str.replace(\"_\", \"\", regex=False)\n",
    ")\n",
    "\n",
    "# Verify we've accounted for all modificaâ€ ions:\n",
    "assert not data[\"Seq\"].str.contains(\"(\", regex=False).sum()\n",
    "\n",
    "# Split the data\n",
    "# The test data contains all of the ProteomeTools sequences.\n",
    "test_df = data.loc[data[\"PT\"], :]\n",
    "data_df = data.loc[~data[\"PT\"], :]\n",
    "\n",
    "# Use 20% of the training set for validation.\n",
    "n_train = int(0.8 * len(data_df))\n",
    "train_df = data_df.iloc[:n_train, :].copy()\n",
    "validation_df = data_df.iloc[n_train:, :].copy()\n",
    "\n",
    "# Print the number in each set: \n",
    "print(\"The training set contains\", len(train_df), \"peptides\")\n",
    "print(\"The validation set contains\", len(validation_df), \"peptides\")\n",
    "print(\"The test set contains\", len(test_df), \"peptides\")\n",
    "print(\"\\nThis is what the training set looks like:\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pyBMWymcSRm"
   },
   "source": [
    "## Create a tokenizer\n",
    "Now that we know the peptides that we want to consider, we need to create a tokenizer that accounts for all of the amino acids and modifications that may be present. The tokenizer will split the peptide strings into the amino acid residues and modifications that comprise them. Fortunately, the `PeptideTokenizer` class has a `from_proforma()` method that allows us to extract the amino acids and modifications from a collection of peptides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "id": "J1l3KylpVdsG",
    "outputId": "57d8e17b-ce29-47a6-c0dc-3f13fc90ce23"
   },
   "outputs": [],
   "source": [
    "# Create the tokenizer:\n",
    "tokenizer = PeptideTokenizer.from_proforma(validation_df[\"Seq\"])\n",
    "\n",
    "# See the amino acid tokens:\n",
    "pd.DataFrame(tokenizer.residues.items(), columns=[\"Token\", \"Mass\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdDWRaWbcSRn"
   },
   "source": [
    "It looks like our tokenizer has captured all of the residues we expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gph1OAyBcSRn"
   },
   "source": [
    "## Preparing Datasets\n",
    "When modeling data using PyTorch, we typically need to pur our data into a PyTorch `Dataset` and serve it to your model using a PyTorch `DataLoader`. \n",
    "Here, we use Depthcharge's `PeptideDataset` class, which handles transforming the peptide strings for modeling. \n",
    "Because this dataset is small from a memory perspective, we go ahead and load it onto the GPU as well, to increase our training speed.\n",
    "\n",
    "We also transform the measured CCS using standard scaling, making it an easier value for the model to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b69y-jG2J61i"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Create a PeptideDataset holding the training data:\n",
    "train_dataset = PeptideDataset(\n",
    "    tokenizer,\n",
    "    train_df[\"Seq\"].to_numpy(), \n",
    "    torch.tensor(train_df[\"Charge\"].to_numpy()),\n",
    "    torch.tensor(scaler.fit_transform(train_df[[\"CCS\"]]).flatten()),\n",
    ")\n",
    "\n",
    "# Create a PeptideDatset containing the validation data:\n",
    "validation_dataset = PeptideDataset(\n",
    "    tokenizer,\n",
    "    validation_df[\"Seq\"].to_numpy(),\n",
    "    torch.tensor(validation_df[\"Charge\"].to_numpy()),\n",
    "    torch.tensor(scaler.transform(validation_df[[\"CCS\"]]).flatten()),\n",
    ")\n",
    "\n",
    "# Create a PeptideDataset containing the test data:\n",
    "test_dataset = PeptideDataset(\n",
    "    tokenizer,\n",
    "    test_df[\"Seq\"].to_numpy(),\n",
    "    torch.tensor(test_df[\"Charge\"].to_numpy()),\n",
    ")\n",
    "\n",
    "# Transfer all of the data to the GPU.\n",
    "# This data is relatively small so it can all live there.\n",
    "# Many datasets won't all fit on the GPU at once though.\n",
    "for dset in (train_dataset, validation_dataset, test_dataset):\n",
    "    tensors = []\n",
    "    for data in dset.tensors:\n",
    "        tensors.append(data.to(\"cuda\"))\n",
    "\n",
    "    dset.tensors = tuple(tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMXoLhWltO1f"
   },
   "source": [
    "Using our datsets, we create the PyTorch DataLoaders that we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZB2jOCItZLe"
   },
   "outputs": [],
   "source": [
    "# Create data loaders to feed data to the model:\n",
    "train_loader = train_dataset.loader(batch_size=128, shuffle=True)\n",
    "validation_loader = validation_dataset.loader(batch_size=1024)\n",
    "test_loader = test_dataset.loader(batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQzUUxE0cSRo"
   },
   "source": [
    "## Create a model\n",
    "\n",
    "Time to create a deep learning model using PyTorch Lightning and Depthcharge! \n",
    "Our model consists of a `PeptideTransformerEncoder` module to embed peptides and a `FeedForward` module to predict CCS from the latent representation. \n",
    "With PyTorch Lightning, we need to specify the modules that comprise our model, define the optimizer(s) we will use to train it, and tell Lightning how to run the model when training, validating, and predicting.\n",
    "\n",
    "For this task, we're trying to minimize the mean squared error (MSE) loss function:\n",
    "$$ L = \\frac{1}{n}\\sum^{n}_{i=1}(Y_i - \\hat{Y}_i)^2$$\n",
    "\n",
    "Where $n$ is the number of peptides, $Y$ is the measured CCS, and $\\hat{Y}_i$ is the predicted CCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XepQ0TPErkY"
   },
   "outputs": [],
   "source": [
    "class CCSPredictor(pl.LightningModule):\n",
    "    \"\"\"A Transformer model for CCS prediction\"\"\"\n",
    "    def __init__(self, tokenizer, d_model, n_layers):\n",
    "        \"\"\"Initialize the CCSPredictor\"\"\"\n",
    "        super().__init__()\n",
    "        self.peptide_encoder = PeptideTransformerEncoder(\n",
    "            n_tokens=tokenizer,\n",
    "            d_model=d_model,\n",
    "            n_layers=n_layers,\n",
    "        )\n",
    "        self.ccs_head = FeedForward(d_model, 1, 3)\n",
    "\n",
    "    def step(self, batch, batch_idx):\n",
    "        \"\"\"A training/validation/inference step.\"\"\"\n",
    "        seqs, charges, *ccs = batch\n",
    "        try:\n",
    "            embedded, _ = self.peptide_encoder(seqs, charges)\n",
    "        except IndexError as err:\n",
    "            print(batch)\n",
    "            raise err\n",
    "\n",
    "        pred = self.ccs_head(embedded[:, 0, :]).flatten()\n",
    "        if ccs:\n",
    "            ccs = ccs[0].type_as(pred)\n",
    "            loss = torch.nn.functional.mse_loss(pred, ccs)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return pred, loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"A training step\"\"\"\n",
    "        _, loss = self.step(batch, batch_idx)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"A validation step\"\"\"\n",
    "        _, loss = self.step(batch, batch_idx)\n",
    "        self.log(\"validation_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        \"\"\"An inference step\"\"\"\n",
    "        pred, _ = self.step(batch, batch_idx)\n",
    "        return pred\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure the optimizer for training.\"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yjzbpCGcSRo"
   },
   "source": [
    "## Prepare the model\n",
    "\n",
    "With our model defined and our data loaders ready to go, its almost time to fit the model to the data.\n",
    "The PyTorch Lightning `Trainer` will handle a lot of the training for us. \n",
    "We enable an early stopping criterium here, so that the trainer will stop once the MSE on our validation dataset stops improving. \n",
    "This model should take ~5 min to train.\n",
    "\n",
    "First we need to create a model and a way to track the loss as the model learns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TYcxB_F9UKGY",
    "outputId": "023d6297-dfa6-47d5-d49d-1f00789e84e5"
   },
   "outputs": [],
   "source": [
    "# Create a model.\n",
    "# If you have time, try changing d_model and n_layers.\n",
    "model = CCSPredictor(tokenizer, d_model=32, n_layers=4)\n",
    "\n",
    "# Create a way to log our losses.\n",
    "class LossLogger(Callback):\n",
    "    def __init__(self):\n",
    "        self.history = []\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        self.history.append(dict(trainer.callback_metrics))\n",
    "\n",
    "# Create our logger:\n",
    "logger = LossLogger()\n",
    "\n",
    "# Create the model trainer.\n",
    "# If you have time, try changing max_epochs\n",
    "trainer = pl.Trainer(callbacks=[logger], max_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJqGQLOKw8nd"
   },
   "source": [
    "## Fit the model\n",
    "\n",
    "Now we're finally ready to fit the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466,
     "referenced_widgets": [
      "28a1dcdb2354420fb471f14465aaf918",
      "f0b1323c326745069607c43152e03fc0",
      "94a1b2206c6040ca96f518da57da2bed",
      "da372b93fa99485c92c7c6858399f3ca",
      "31b734a40fb84f4188371800c1b483e3",
      "774d3e3dba344f27b6a194a263ad19fa",
      "a230b7175fc344c082785de6b63239d9",
      "ef80806bfb454a76b140882e280fdbc6",
      "fbf87f5b252a4586913acefe0e20208e",
      "2473e9f04855401c964b063b85affb21",
      "6c8f6328253f491da6aa443ac59ff56f",
      "472e90fbb91e4983a22e25699288ea4a",
      "56bfa26caab048c6b2d82eeaac971ce0",
      "56c5bab95cf647edb21d8a5c13b78acc",
      "ee11d9714b3847f1b714cbc8a9202999",
      "a72aa43a09944dabbe4e8a186f413c99",
      "8a3566841b82422eb3d690cf2aef1515",
      "aa7ce1ff4dca48b585e5ccd4b7fd5fab",
      "8489f5d66a40414891dd0aff60c227e3",
      "b44a6eeb6c104e8bb590da1627d97c6d",
      "8c849a553f164faf951565f082fc02a6",
      "d29e663c74734a63966e3ec5187c299a",
      "b18db0f1abb6450ba16c402949d30a9a",
      "e2c9ae6074f141e5bd7576601a2140b4",
      "128fff56b72446b4bcb64884a77125f7",
      "ff01d2703633433c8d891131df9ea8b5",
      "39c3ae458813496dba5de96886072d06",
      "d889f5ecbbf64e6db5fcea9f7b9b175b",
      "ded964ff7b8748e3b16d8dfa26ff3b54",
      "c51c475698314e6ab84d14d909c11a65",
      "dfe95f9d26a943149238ba6d1581377e",
      "4d4d11b15efd4616bae5b13467afba86",
      "3435b74fd42e4fa4ae35f6bcddd1dd27",
      "8618e622e09442a8a784822d4ccb635b",
      "e870f4fbb4b7497cbc9a91997fdc61f6",
      "02a753bc91164de18fe271d86ff005c0",
      "03dedc53d63142889b254efe07596929",
      "1dcdfb6da751401494b5d5488c15ad5c",
      "82a66d13913b491185daa999e659c1d4",
      "59fab7c07dd84d0eb091b86bc6829549",
      "b84904a7720242d9a5f6d5116ffb0cb0",
      "1685ecb447084b05b4baa87f268e2e00",
      "aa434ca9ce6d4e7ba0d5275f3fac5d3d",
      "5a3494c816ca44ac91454db63cb8d4f2",
      "22140f141bc7405f80445f789bef64da",
      "9b6c71f901d044aea5f7c0b00bcfde41",
      "4ec81d79a76e438eada2daf00cdd7676",
      "ff999d2aaeeb47a7a2a7bd3ca8c5d797",
      "6c7cf96f3b69492f939b70f88ee893ae",
      "3b18834763e447a0993d54e9892caab6",
      "f81adde5b22040168752106227ac38e6",
      "25a4928dcbf94d26a1831c91c09d453d",
      "65a3d5bfaacd4b99ba1cd7a9c7c28ad8",
      "ac5edd3e15f14b13a311e242b10ad34d",
      "f71c3159fd5e4bb5b45dd20adf1c0a70"
     ]
    },
    "id": "blGT2qXlw6ck",
    "outputId": "0ee04830-9a1a-49c6-a0b5-27b5cdd4e1a0"
   },
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model=model, \n",
    "    train_dataloaders=train_loader,     \n",
    "    val_dataloaders=validation_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2xdPLpduTwb"
   },
   "source": [
    "Congratulations! You just trained a deep learning model. \n",
    "\n",
    "Let's take a look at the loss curves from training now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "pYmMBvn9mUZU",
    "outputId": "a5231baf-b5c3-4b56-fddc-eb27caa11db6"
   },
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(logger.history)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses[\"train_loss\"], label=\"Training MSE\")\n",
    "plt.plot(losses[\"validation_loss\"], label=\"Validation MSE\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15hKlnLWcSRq"
   },
   "source": [
    "## Predict on the Validation dataset\n",
    "\n",
    "We now want to see how we've done, aside from just looking at the MSE. \n",
    "To get the predicted CCS for every peptide in our validation set, we use the `predict()` method for the trainer on our validation data loader.\n",
    "We then visualize our results using a hexbin plot, which in this case si like a scatterplot + heatmap all in one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533,
     "referenced_widgets": [
      "bfd6441b451f4f53b5248e089fd05656",
      "630b2d0b774a47a1ad60fbc9d82c528e",
      "add71e51528b459f926e768d416c58ba",
      "97b4640bd2dc43a1ad8f26930f59b6c3",
      "410477fc18a34f51993df77918e2fec7",
      "532c328212844e5697275b20f83b5552",
      "5ae2ef97d5f34d36b0600c4f77fdf1a8",
      "03a2e77adae145bb88c760eb0992bc30",
      "d12e8ecb56cb4d60b77eb00b9f9cb8bc",
      "a6867f1f7fd841c09266ebc7da07c146",
      "c893548eb42940ceafd6a66960494683"
     ]
    },
    "id": "k_2PrF_QffZG",
    "outputId": "3dc0c0d3-4590-43e4-df47-cda9de8a0f9f"
   },
   "outputs": [],
   "source": [
    "pred = trainer.predict(model, validation_loader)\n",
    "validation_df = validation_df.copy()\n",
    "validation_df[\"pred\"] = scaler.inverse_transform(\n",
    "    torch.cat(pred).detach().cpu().numpy()[:, None]\n",
    ").flatten()\n",
    "\n",
    "plt.viridis()\n",
    "plt.figure()\n",
    "plt.hexbin(\n",
    "    validation_df[\"CCS\"], \n",
    "    validation_df[\"pred\"],\n",
    "    mincnt=1, \n",
    "    gridsize=200, \n",
    "    bins=\"log\", \n",
    ")\n",
    "plt.axis('equal')    \n",
    "plt.xlabel(\"Measured CCS\")\n",
    "plt.ylabel(\"Predicted CCS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fIL4VXxcSRq"
   },
   "source": [
    "This looks pretty good to me. \n",
    "If we want to perform further tweaks and optimizations, we should turn back and do them now. \n",
    "If not, we're ready to get our predictions for the test set, after which we should cease trying to optimize our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiKcu1rscSRq"
   },
   "source": [
    "## Predict on the Test dataset\n",
    "\n",
    "Like with our validation data, we use the `predict()` method to get the predicted CCS for each of our test dataset peptides. When you're sure that your ready to proceed to the test set, go ahead and delete the first line in the cell below, then run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "Vo6-0ropJrAT",
    "outputId": "40ca17e8-9f48-45ec-bd41-b0be752ed898"
   },
   "outputs": [],
   "source": [
    "raise RuntimeError(\"Are you sure your ready to run this?\")\n",
    "\n",
    "trainer = pl.Trainer()\n",
    "pred = trainer.predict(model, test_loader)\n",
    "\n",
    "test_df = test_df.copy()\n",
    "test_df[\"pred\"] = scaler.inverse_transform(\n",
    "    torch.cat(pred).detach().cpu().numpy()[:, None]\n",
    ").flatten()\n",
    "\n",
    "plt.viridis()\n",
    "plt.figure()\n",
    "plt.hexbin(\n",
    "    test_df[\"CCS\"], \n",
    "    test_df[\"pred\"],\n",
    "    mincnt=1, \n",
    "    gridsize=200, \n",
    "    bins=\"log\", \n",
    ")\n",
    "plt.axis('equal')    \n",
    "plt.xlabel(\"Measured CCS\")\n",
    "plt.ylabel(\"Predicted CCS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lcwS5iocSRr"
   },
   "source": [
    "Nice! This looks great. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
