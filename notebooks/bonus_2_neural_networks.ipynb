{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00150c96-2537-470b-855b-dfcca475778d",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "\n",
    "In this notebook, you will implement fundamental neural network architectures from scratch using NumPy and PyTorch.\n",
    "These exercises will deepen your understanding of how neural networks work, from basic matrix multiplications to more complex architectures like convolutional neural networks (CNNs).\n",
    "As part this notebook, you will build and train neural networks, visualize their learning process, and apply them to real-world biomedical datasets.\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "- Understand neural networks at a fundamental level.\n",
    "- Apply and extend neural network architectures.\n",
    "- Explore real-world applications with CNNs.\n",
    "- Learn practical deep learning skills with PyTorch.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "You will need the following Python packages to complete this notebook:\n",
    "\n",
    "- [NumPy](https://numpy.org/): for numerical computing.\n",
    "- [Matplotlib](https://matplotlib.org/): for data visualization.\n",
    "- [MedMNIST](https://medmnist.com/): dataset of biomedical images.\n",
    "- [PyTorch](https://pytorch.org/): a commonly used framework for neural networks.\n",
    "\n",
    "You can install these packages by running:\n",
    "\n",
    "```\n",
    "pip install numpy matplotlib medmnist torch torchvision\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4a23b6-6877-4869-b9eb-f1f217d4aa9a",
   "metadata": {},
   "source": [
    "## Neural networks as matrix multiplications\n",
    "\n",
    "At the core of a neural network is a series of layers that consist of multiple nodes (or neurons), each performing simple calculations and passing information to the next layer.\n",
    "In a fully-connected neural network, each neuron is connected to the neurons in the adjacent layers.\n",
    "The connections are represented by weights.\n",
    "When the input data is passed through the network, these weights are used to compute the activations of each neuron in the following way:\n",
    "\n",
    "For a single layer, the computation at each neuron is:\n",
    "$$\n",
    "z = W \\cdot x + b\n",
    "$$\n",
    "where:\n",
    "- $W$ is the weight matrix.\n",
    "- $x$ is the input vector (either the input data for the first layer or the activations from the previous layer).\n",
    "- $b$ is the bias term (added to each neuron to control the output even when the input is zero).\n",
    "- $z$ is the linear combination of the inputs (pre-activation value).\n",
    "\n",
    "This operation is essentially a **matrix multiplication** between the input vector and the weight matrix, followed by the addition of the bias term. The matrix multiplication allows the network to process multiple inputs at once, making neural networks computationally efficient.\n",
    "\n",
    "## Non-linear activation functions\n",
    "\n",
    "After the matrix multiplication step, neural networks apply **non-linear activation functions** to the pre-activation values $z$ to introduce non-linearity into the model.\n",
    "Without these non-linear activations, the neural network would behave like a simple linear regression model, regardless of the number of layers.\n",
    "\n",
    "Commonly used activation functions include:\n",
    "\n",
    "- ReLU (rectified linear unit): The ReLU function outputs the input directly if it's positive, and outputs zero otherwise.\n",
    "$$\n",
    "\\text{ReLU}(z) = \\max(0, z)\n",
    "$$\n",
    "ReLU is widely used due to its simplicity and its ability to alleviate the vanishing gradient problem in deep networks.\n",
    "\n",
    "- Sigmoid: The sigmoid function squashes the input into the range (0, 1), making it useful for binary classification tasks.\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "This function maps large negative inputs to values close to 0 and large positive inputs to values close to 1.\n",
    "\n",
    "- Tanh (hyperbolic tangent): The tanh function squashes the input into the range (-1, 1). It is similar to the sigmoid function but outputs values symmetrically around zero.\n",
    "$$\n",
    "\\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\n",
    "$$\n",
    "\n",
    "## Multi-layer neural networks\n",
    "\n",
    "A multi-layer neural network, often referred to as a **feedforward neural network** or **multi-layer perceptron (MLP)**, consists of multiple layers of neurons:\n",
    "\n",
    "1. **Input layer**: Receives the input data. Each neuron corresponds to a feature in the data.\n",
    "2. **Hidden layers**: These layers lie between the input and output layers. Each hidden layer performs matrix multiplication followed by a non-linear activation function. A neural network can have multiple hidden layers, allowing it to capture more complex relationships.\n",
    "3. **Output layer**: Produces the final predictions. For binary classification, the output layer typically has one neuron with a sigmoid activation function. For multi-class classification, the output layer might have multiple neurons with a softmax activation function.\n",
    "\n",
    "### Forward propagation through multiple layers\n",
    "\n",
    "In a multi-layer neural network, each layer transforms the input data and passes it to the next layer.\n",
    "This process is called **forward propagation**.\n",
    "\n",
    "For a network with one hidden layer, the forward propagation looks like this:\n",
    "\n",
    "1. **Input to hidden layer**:\n",
    "$$\n",
    "z^{(1)} = W^{(1)} \\cdot x + b^{(1)}\n",
    "$$\n",
    "$$\n",
    "a^{(1)} = \\text{activation}(z^{(1)})\n",
    "$$\n",
    "\n",
    "2. **Hidden layer to output layer**:\n",
    "$$\n",
    "z^{(2)} = W^{(2)} \\cdot a^{(1)} + b^{(2)}\n",
    "$$\n",
    "$$\n",
    "a^{(2)} = \\text{activation}(z^{(2)})\n",
    "$$\n",
    "   \n",
    "Here, $W^{(1)}$ and $W^{(2)}$ are the weight matrices for the first and second layers, $b^{(1)}$ and $b^{(2)}$ are the bias vectors, and $a^{(1)}$ and $a^{(2)}$ are the activations of the hidden and output layers.\n",
    "\n",
    "This chain of matrix multiplications and activations continues through all the layers of the network.\n",
    "\n",
    "### Learning in multi-layer networks: Backpropagation\n",
    "\n",
    "The goal of training a neural network is to adjust the weights and biases so that the predictions closely match the actual targets.\n",
    "This is done using an algorithm called **backpropagation**.\n",
    "Backpropagation involves the following steps:\n",
    "\n",
    "1. **Forward pass**: Compute the network's output by performing matrix multiplication and activation functions layer by layer.\n",
    "2. **Compute loss**: Calculate how far off the network's predictions are from the actual targets using a loss function (e.g., mean squared error for regression, cross-entropy for classification).\n",
    "3. **Backward pass (backpropagation)**: Compute the gradients of the loss with respect to the weights and biases using the chain rule of calculus.\n",
    "4. **Gradient descent**: Update the weights and biases using the gradients to minimize the loss function.\n",
    "\n",
    "This process is repeated for multiple epochs, gradually improving the performance of the network.\n",
    "\n",
    "### Example of forward propagation through multiple layers\n",
    "\n",
    "Consider a neural network with the following architecture:\n",
    "- Input layer: 2 neurons (for a 2D input).\n",
    "- Hidden layer: 3 neurons with ReLU activation.\n",
    "- Output layer: 1 neuron with sigmoid activation (for binary classification).\n",
    "\n",
    "1. Input to hidden layer:\n",
    "$$\n",
    "z^{(1)} = W^{(1)} \\cdot x + b^{(1)}, \\quad a^{(1)} = \\text{ReLU}(z^{(1)})\n",
    "$$\n",
    "where:\n",
    "$$\n",
    "W^{(1)} = \\begin{bmatrix}\n",
    "w_{11}^{(1)} & w_{12}^{(1)} \\\\\n",
    "w_{21}^{(1)} & w_{22}^{(1)} \\\\\n",
    "w_{31}^{(1)} & w_{32}^{(1)}\n",
    "\\end{bmatrix}, \\quad\n",
    "x = \\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "2. Hidden layer to output layer:\n",
    "$$\n",
    "z^{(2)} = W^{(2)} \\cdot a^{(1)} + b^{(2)}, \\quad a^{(2)} = \\text{Sigmoid}(z^{(2)})\n",
    "$$\n",
    "\n",
    "Through this series of operations, the input is transformed into a prediction.\n",
    "\n",
    "---\n",
    "\n",
    "In summary, multi-layer neural networks use a combination of matrix multiplications and non-linear activation functions to learn complex relationships in data.\n",
    "The power of these networks lies in their ability to stack multiple layers of transformations, enabling them to model highly complex patterns.\n",
    "The learning process is guided by backpropagation and gradient descent, which iteratively adjust the weights and biases to minimize the prediction error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62f43f1-6d49-4b3a-9d1a-7f5da2776ffa",
   "metadata": {},
   "source": [
    "## Task: Neural networks from scratch using NumPy\n",
    "\n",
    "In this first exercise, you will create a fully-connected neural network from scratch using NumPy.\n",
    "This will give you a deep understanding of how neural networks work at the fundamental level, without relying on high-level frameworks.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Implement a simple feedforward neural network with one hidden layer.\n",
    "- The architecture will be:\n",
    "    - Input layer: 2 neurons (for a 2D synthetic dataset).\n",
    "    - Hidden layer: 3 neurons with ReLU activation.\n",
    "    - Output layer: 1 neuron (binary classification) with sigmoid activation.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "- Initialize weights: Write a function that initializes the weights and biases randomly. Use small random values to ensure the network starts with different weights.\n",
    "- Forward propagation: Implement forward propagation using matrix multiplication to calculate the activations of each layer.\n",
    "- Loss function: Use binary cross-entropy as the loss function.\n",
    "- Backpropagation: Write the backpropagation algorithm to compute the gradients of the loss with respect to the weights.\n",
    "- Gradient descent: Update the weights using the gradients computed during backpropagation and a predefined learning rate.\n",
    "- Training: Train the network on a small synthetic dataset for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9dabc4-8cf4-4e05-b182-49f69d02d5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Step 1: Generate synthetic data.\n",
    "def generate_data(num_points, noise=0.1):\n",
    "    np.random.seed(42)\n",
    "    # Random points between -1 and 1.\n",
    "    X = np.random.rand(num_points, 2) * 2 - 1\n",
    "    # Label: 1 if product of coordinates > 0, else 0.\n",
    "    Y = (X[:, 0] * X[:, 1] > 0).astype(int).reshape(-1, 1)\n",
    "    # Add some noise.\n",
    "    Y = (Y + (np.random.rand(num_points, 1) < noise).astype(int)) % 2\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# Step 2: Initialize weights.\n",
    "def initialize_weights(input_size, hidden_size, output_size):\n",
    "    np.random.seed(42)\n",
    "    W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "    b1 = np.zeros((1, hidden_size))\n",
    "    W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "    b2 = np.zeros((1, output_size))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "\n",
    "# Step 3: Forward propagation.\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    # TODO\n",
    "    Z1 = None  # Apply W1 to input.\n",
    "    A1 = None  # ReLU activation.\n",
    "    Z2 = None  # Apply W2 to A1.\n",
    "    A2 = None  # Sigmoid activation.\n",
    "    return A1, A2\n",
    "\n",
    "\n",
    "# Step 4: Compute loss.\n",
    "def compute_loss(A2, Y):\n",
    "    # TODO\n",
    "    loss = None\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Step 5: Backpropagation.\n",
    "def backpropagation(X, Y, A1, A2, W2):\n",
    "    m = X.shape[0]\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = np.dot(A1.T, dZ2) / m\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "    dZ1 = np.dot(dZ2, W2.T) * (A1 > 0)\n",
    "    dW1 = np.dot(X.T, dZ1) / m\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "\n",
    "# Step 6: Gradient descent.\n",
    "def gradient_descent(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    # TODO\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "\n",
    "# Step 7: Training loop.\n",
    "def train(X, Y, hidden_size, learning_rate, n_epochs):\n",
    "    input_size = X.shape[1]\n",
    "    output_size = 1\n",
    "    W1, b1, W2, b2 = initialize_weights(input_size, hidden_size, output_size)\n",
    "\n",
    "    loss_history = []\n",
    "    for i in range(n_epochs):\n",
    "        A1, A2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "\n",
    "        loss = compute_loss(A2, Y)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        dW1, db1, dW2, db2 = backpropagation(X, Y, A1, A2, W2)\n",
    "\n",
    "        W1, b1, W2, b2 = gradient_descent(\n",
    "            W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate\n",
    "        )\n",
    "\n",
    "        # Print the loss every 100 epochs.\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch {i:3d}: Loss = {loss:.3f}\")\n",
    "\n",
    "    return W1, b1, W2, b2, loss_history\n",
    "\n",
    "\n",
    "# Step 8: Predictions.\n",
    "def predict(X, W1, b1, W2, b2):\n",
    "    _, A2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "    predictions = (A2 > 0.5).astype(int)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Generate synthetic data.\n",
    "X, Y = generate_data(num_points=200, noise=0.1)\n",
    "\n",
    "# Train the neural network.\n",
    "# TODO\n",
    "W1, b1, W2, b2, loss_history = None\n",
    "\n",
    "# Make predictions (on the training data—not correct).\n",
    "predictions = predict(X, W1, b1, W2, b2)\n",
    "accuracy = np.mean(predictions == Y) * 100\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Plot the loss history.\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(loss_history)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot the decision boundary.\n",
    "x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "Z = predict(np.c_[xx.ravel(), yy.ravel()], W1, b1, W2, b2)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.contourf(xx, yy, Z, cmap=\"RdBu\", alpha=0.5)\n",
    "ax.scatter(X[:, 0], X[:, 1], c=Y[:, 0], cmap=\"RdBu\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c3b017-dbc0-4f61-8e6d-50cfc74e61ca",
   "metadata": {},
   "source": [
    "## Task: Extend the neural network with multiple layers (NumPy)\n",
    "\n",
    "Now that you have implemented a simple neural network, it is time to extend it to a deeper architecture by adding more hidden layers.\n",
    "This will help you understand how deep learning models extract complex features from data.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Modify your previous implementation to support an arbitrary number of layers.\n",
    "- The architecture will now be:\n",
    "    - Input layer: 2 neurons.\n",
    "    - Multiple hidden layers with ReLU activation.\n",
    "    - Output layer: 1 neuron with sigmoid activation.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "- Modify forward propagation: Extend your forward propagation function to handle multiple layers.\n",
    "- Modify backpropagation: Update the backpropagation function to calculate gradients for all layers.\n",
    "- Activation functions: Experiment with different activation functions like sigmoid and tanh for hidden layers.\n",
    "- Training: Train your deeper network on the same synthetic dataset and compare the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e16a8a0-fecd-41b5-b1ba-859fe0d28c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights_deep(layer_dims):\n",
    "    np.random.seed(42)\n",
    "    weights = {}\n",
    "    n_layers = len(layer_dims)\n",
    "    for i in range(1, n_layers):\n",
    "        # He initialization for ReLU.\n",
    "        weights[f\"W{i}\"] = np.random.randn(layer_dims[i - 1], layer_dims[i]) * np.sqrt(\n",
    "            2 / layer_dims[i - 1]\n",
    "        )\n",
    "        weights[f\"b{i}\"] = np.zeros((1, layer_dims[i]))\n",
    "    return weights\n",
    "\n",
    "\n",
    "def forward_propagation_deep(X, weights):\n",
    "    caches = {}\n",
    "    A = X\n",
    "    n_layers = len(weights) // 2\n",
    "\n",
    "    # Iterate through all layers (except the last).\n",
    "    for i in range(1, n_layers):\n",
    "        # TODO\n",
    "        # Z = None\n",
    "        # A = None\n",
    "        caches[f\"Z{i}\"] = Z\n",
    "        caches[f\"A{i}\"] = A\n",
    "\n",
    "    # Output layer (uses sigmoid).\n",
    "    # TODO\n",
    "    ZL = None\n",
    "    AL = None\n",
    "    caches[f\"Z{n_layers}\"] = ZL\n",
    "    caches[f\"A{n_layers}\"] = AL\n",
    "\n",
    "    return AL, caches\n",
    "\n",
    "\n",
    "def backpropagation_deep(X, Y, weights, caches):\n",
    "    grads = {}\n",
    "    m = X.shape[0]\n",
    "    n_layers = len(weights) // 2\n",
    "\n",
    "    # Output layer gradients.\n",
    "    dZL = caches[f\"A{n_layers}\"] - Y\n",
    "    grads[f\"dW{n_layers}\"] = np.dot(caches[f\"A{n_layers - 1}\"].T, dZL) / m\n",
    "    grads[f\"db{n_layers}\"] = np.sum(dZL, axis=0, keepdims=True) / m\n",
    "\n",
    "    # Backpropagation through remaining layers.\n",
    "    for i in reversed(range(1, n_layers)):\n",
    "        dZ = np.dot(dZL, weights[f\"W{i+1}\"].T) * (caches[f\"A{i}\"] > 0)\n",
    "        grads[f\"dW{i}\"] = np.dot(X.T if i == 1 else caches[f\"A{i - 1}\"].T, dZ) / m\n",
    "        grads[f\"db{i}\"] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "        dZL = dZ\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "def gradient_descent_deep(weights, grads, learning_rate):\n",
    "    # TODO\n",
    "    return weights\n",
    "\n",
    "\n",
    "def train_deep(X, Y, layer_dims, learning_rate, n_epochs):\n",
    "    weights = initialize_weights_deep(layer_dims)\n",
    "\n",
    "    loss_history = []\n",
    "    for i in range(n_epochs):\n",
    "        AL, caches = forward_propagation_deep(X, weights)\n",
    "\n",
    "        loss = compute_loss(AL, Y)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        grads = backpropagation_deep(X, Y, weights, caches)\n",
    "\n",
    "        weights = gradient_descent_deep(weights, grads, learning_rate)\n",
    "\n",
    "        # Print the loss every 100 epochs.\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch {i:3d}: Loss = {loss:.3f}\")\n",
    "\n",
    "    return weights, loss_history\n",
    "\n",
    "\n",
    "def predict_deep(X, weights):\n",
    "    AL, _ = forward_propagation_deep(X, weights)\n",
    "    predictions = (AL > 0.5).astype(int)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Generate synthetic data.\n",
    "X, Y = generate_data(num_points=200, noise=0.1)\n",
    "\n",
    "# Define the layer dimensions for a deep network (input layer -> hidden layers -> output layer).\n",
    "layer_dims = [2, 5, 5, 1]  # 2 inputs, 2 hidden layers with 5 neurons each, 1 output.\n",
    "\n",
    "# Train the deep neural network.\n",
    "weights, loss_history = train_deep(X, Y, layer_dims, learning_rate=1.0, n_epochs=1000)\n",
    "\n",
    "# Make predictions (on the training data—not correct).\n",
    "predictions = predict_deep(X, weights)\n",
    "accuracy = np.mean(predictions == Y) * 100\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Plot the loss history.\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(loss_history)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Plot the decision boundary.\n",
    "x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "Z = predict_deep(np.c_[xx.ravel(), yy.ravel()], weights)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.contourf(xx, yy, Z, cmap=\"RdBu\", alpha=0.5)\n",
    "ax.scatter(X[:, 0], X[:, 1], c=Y[:, 0], cmap=\"RdBu\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d29dd9e-5077-4e6d-8744-30f0a58b3d4b",
   "metadata": {},
   "source": [
    "**Discuss what happens when you change the number of neurons, the number of layers, and the activation functions.**\n",
    "\n",
    "Write your observations here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5791e9-e0c5-414d-9c94-b25e5e4df356",
   "metadata": {},
   "source": [
    "## Introduction to convolutional neural networks (CNNs)\n",
    "\n",
    "Convolutional neural networks (CNNs) are a specialized type of neural network primarily used for processing data that has a grid-like structure, such as images.\n",
    "CNNs have proven to be highly effective in tasks like image classification, object detection, and medical image analysis, among many others.\n",
    "\n",
    "In traditional fully-connected neural networks, each neuron in one layer is connected to every neuron in the next layer.\n",
    "While this works well for small datasets, it becomes impractical for high-dimensional data like images.\n",
    "For example, a 128x128 grayscale image has 16,384 pixels, and a fully-connected neural network would require an enormous number of parameters if each pixel were connected to every neuron in the first layer.\n",
    "\n",
    "CNNs take advantage of the hierarchical and spatial structure of images to reduce the number of parameters while preserving important features like edges, textures, and shapes.\n",
    "\n",
    "### Basic building blocks of CNNs\n",
    "\n",
    "CNNs consist of three main types of layers: convolutional layers, pooling layers, and fully-connected layers.\n",
    "\n",
    "#### Convolutional layers\n",
    "\n",
    "The **convolutional layer** is the core building block of a CNN.\n",
    "Instead of connecting every input pixel to every output neuron, a convolutional layer applies a set of filters (also called kernels) that slide over the input image.\n",
    "Each filter detects specific features, like edges or textures, by performing an operation called convolution.\n",
    "\n",
    "How convolution works:\n",
    "- Each filter has a small size (e.g., 3x3 or 5x5) compared to the full image.\n",
    "- The filter slides over the input image, and at each position, it computes the dot product between the filter and the corresponding region of the image.\n",
    "- This produces an **activation map** or **feature map**, which highlights the presence of certain features (e.g., edges or corners) in the image.\n",
    "\n",
    "For example, consider a 3x3 filter:\n",
    "$$\n",
    "\\text{Filter} = \\begin{bmatrix}\n",
    "f_{11} & f_{12} & f_{13} \\\\\n",
    "f_{21} & f_{22} & f_{23} \\\\\n",
    "f_{31} & f_{32} & f_{33}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "and a region of the image:\n",
    "$$\n",
    "\\text{Image region} = \\begin{bmatrix}\n",
    "p_{11} & p_{12} & p_{13} \\\\\n",
    "p_{21} & p_{22} & p_{23} \\\\\n",
    "p_{31} & p_{32} & p_{33}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The convolution operation is the element-wise multiplication and summation of these two matrices:\n",
    "$$\n",
    "\\text{Convolution output} = (f_{11} \\cdot p_{11}) + (f_{12} \\cdot p_{12}) + \\dots + (f_{33} \\cdot p_{33})\n",
    "$$\n",
    "\n",
    "This operation is repeated as the filter \"slides\" across the entire image, generating a feature map.\n",
    "\n",
    "#### Pooling layers\n",
    "\n",
    "After a convolutional layer, it is common to apply a **pooling layer** to reduce the spatial dimensions of the feature maps and down-sample the data.\n",
    "Pooling helps reduce the number of parameters, making the model computationally efficient and less prone to overfitting.\n",
    "\n",
    "Types of ooling:\n",
    "- **Max pooling**: Takes the maximum value from a set of pixels within a pooling window (e.g., a 2x2 window). Max pooling captures the most prominent features.\n",
    "- **Average pooling**: Computes the average of the values within the pooling window. Average pooling is less commonly used in modern CNNs compared to max pooling.\n",
    "\n",
    "For example, with max pooling and a 2x2 window, the pooling layer selects the maximum value from each 2x2 region:\n",
    "$$\n",
    "\\text{Image patch} = \\begin{bmatrix}\n",
    "1 & 3 \\\\\n",
    "2 & 4\n",
    "\\end{bmatrix} \\quad \\text{Max Pooling} \\Rightarrow 4\n",
    "$$\n",
    "\n",
    "#### Fully-connected layers\n",
    "\n",
    "After several convolutional and pooling layers, the high-level features extracted by the CNN are passed to **fully-connected layers**, just like in traditional neural networks.\n",
    "These layers combine all the learned features to make predictions (e.g., classifying an image).\n",
    "\n",
    "#### Activation functions in CNNs\n",
    "\n",
    "As in fully-connected neural networks, CNNs use non-linear activation functions like **ReLU** and **sigmoid** to introduce non-linearity after convolution and fully-connected layers.\n",
    "The most common activation function in modern CNNs is ReLU, which is used after each convolutional operation:\n",
    "$$\n",
    "\\text{ReLU}(z) = \\max(0, z)\n",
    "$$\n",
    "\n",
    "### CNN architecture example\n",
    "\n",
    "A typical CNN architecture for image classification might consist of:\n",
    "\n",
    "1. **Input layer**: The input is an image, often represented as a 3D matrix (e.g., width x height x channels). For grayscale images, the number of channels is 1, and for RGB images, it's 3.\n",
    "   \n",
    "2. **Convolutional layer**: Applies filters to detect features like edges or textures. The output is a feature map.\n",
    "   \n",
    "3. **Pooling layer**: Down-samples the feature map to reduce the spatial dimensions and computation.\n",
    "   \n",
    "4. **Convolutional + pooling layers**: Often, multiple convolutional and pooling layers are stacked to progressively detect higher-level features (e.g., shapes, objects).\n",
    "   \n",
    "5. **Fully-connected layers**: After the convolution and pooling layers, the feature maps are flattened and passed through fully-connected layers. These layers combine all the learned features to predict the class label.\n",
    "   \n",
    "6. **Output layer**: For binary classification tasks, the output layer typically has one neuron with a sigmoid activation. For multi-class classification, it would have as many neurons as there are classes, often with a softmax activation.\n",
    "\n",
    "### Key concepts in CNNs\n",
    "\n",
    "#### Stride and padding\n",
    "\n",
    "- **Stride**: The number of pixels by which the filter moves over the input image. A larger stride reduces the spatial size of the output feature map.\n",
    "- **Padding**: Sometimes, the input image is padded with zeros around the borders to preserve the spatial dimensions after convolution. This is called **same padding**. Without padding, the output feature map will be smaller than the input.\n",
    "\n",
    "#### Filters and feature maps\n",
    "\n",
    "- A CNN can have multiple filters in each convolutional layer, where each filter detects different features. For example, one filter might learn to detect horizontal edges, while another might detect vertical edges.\n",
    "- The **depth** of the feature map corresponds to the number of filters in the convolutional layer.\n",
    "\n",
    "### Training a CNN\n",
    "\n",
    "CNNs are trained in a similar way to fully-connected neural networks, using backpropagation and gradient descent.\n",
    "The difference lies in how the gradients are calculated through the convolutional and pooling layers.\n",
    "During training, the network learns the optimal values for the filters that best detect the relevant features for the task at hand.\n",
    "\n",
    "---\n",
    "\n",
    "In summary, CNNs use convolutional and pooling layers to extract hierarchical features from grid-like data, such as images.\n",
    "By using small, local connections (filters), CNNs drastically reduce the number of parameters compared to fully connected networks, making them more efficient and capable of learning complex patterns in visual data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b983db-5fcb-43f3-b450-28d32a7c7d23",
   "metadata": {},
   "source": [
    "## Convolutional neural network for medical image classification (PyTorch)\n",
    "\n",
    "In this exercise, you will switch to PyTorch and implement a CNN to classify medical images.\n",
    "CNNs are powerful models for tasks involving image data, such as detecting diseases from medical images like X-rays.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- You will use a preprocessed dataset of chest X-rays from MedMNIST.\n",
    "    - Load the dataset using PyTorch's DataLoader and apply necessary transformations like resizing and normalization.\n",
    "- Implement CNN architecture\n",
    "    - Define a CNN with multiple convolutional layers followed by pooling layers and fully-connected layers.\n",
    "    - Use ReLU activations and softmax at the output for classification.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "- Data loading: Write code to load the chest X-ray dataset, applying transformations like normalization.\n",
    "- CNN model: Implement a CNN with PyTorch's `nn.Module`. Include several convolutional layers followed by pooling and fully-connected layers.\n",
    "- Training: Train the CNN on the dataset, optimizing with gradient descent (you can use Adam as the optimizer).\n",
    "- Evaluation: Evaluate the CNN's performance on a test set using accuracy, precision, and recall metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e3fdad-38f4-41e0-be01-599379c4869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from medmnist import PneumoniaMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# Data loading and preprocessing.\n",
    "# Apply the following transformations:\n",
    "# - Convert the images to tensors.\n",
    "# - Normalize the images (mean 0.5, std 0.5 for grayscale images).\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "    ]\n",
    ")\n",
    "# Download the dataset (chest X-ray images from Medical MNIST).\n",
    "train_dataset = PneumoniaMNIST(split=\"train\", transform=transform, download=True)\n",
    "test_dataset = PneumoniaMNIST(split=\"test\", transform=transform, download=True)\n",
    "# DataLoader to feed the images into the model in batches.\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "## TODO: Perform some data exploration by visualizing the images, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e22f36d-6ff0-4aee-881f-6f48f5f64fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN model for binary classification.\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Convolutional layer 1: 1 input channel (grayscale), 32 output channels, kernel size 3.\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        # Convolutional layer 2: 32 input channels, 64 output channels, kernel size 3.\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=3, padding=1\n",
    "        )\n",
    "        # Max pooling layer: kernel size 2x2.\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Fully connected Layer 1.\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # Flattened size from conv layer.\n",
    "        # Fully connected layer 2 (output layer): 2 classes for PneumoniaMNIST.\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Implement the forward pass. Apply the layers as follows:\n",
    "        # 1. conv1 -> relu -> pool\n",
    "        # 2. conv2 -> relu -> pool\n",
    "        # TODO\n",
    "        # Flatten the image into a 1D vector for fully connected layers.\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # 3. fc1 -> relu\n",
    "        # TODO\n",
    "        # 4. fc2 (output layer) -> sigmoid\n",
    "        # TODO\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the model.\n",
    "model = CNN()\n",
    "# Binary cross-entropy loss function.\n",
    "criterion = nn.BCELoss()\n",
    "# Adam optimizer with learning rate 0.001.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model.\n",
    "n_epochs = 20\n",
    "train_loss = []\n",
    "# Set the model in training mode.\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Zero the gradients.\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass.\n",
    "        outputs = model(images)\n",
    "        # Compute loss.\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        # Backpropagation.\n",
    "        loss.backward()\n",
    "        # Update weights.\n",
    "        optimizer.step()\n",
    "        # Accumulate the loss.\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Print statistics at the end of every epoch.\n",
    "    epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "    train_loss.append(epoch_loss)\n",
    "\n",
    "# Evaluate the model.\n",
    "# Set the model to evaluation mode.\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        predicted = (outputs > 0.5).int()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy of the network on the test images: {accuracy:.2%}\")\n",
    "# TODO: Calculate the precursor and recall on the test set.\n",
    "precision = None\n",
    "recall = None\n",
    "\n",
    "# Plot training loss over epochs.\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_loss)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
